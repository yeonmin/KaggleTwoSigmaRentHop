{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.stats import boxcox\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 파생 변수를 생성하여 새로운 Input Data Set을 만드는 코드\n",
    "2. 학습하는 코드에서는 여기서 만든 Input Data Set을 사용하여 바로바로 학습함\n",
    "3. 따로 전처리하는 부분과 학습하는 부분을 분리한 이유는 전처리하는 데도 시간이 10분정도 소요되고 전처리하여 나온 Input Data Set을 여러가지 다른 알고리즘에서 학습하기 위하여 따로 분리함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Data Set을 새롭게 만들어 놓는 이유는 Image나 Building ID, Price정보는 처리하는데 시간이 오래걸리기 때문에 결과가 나오고 나서 그 결과를 따로 Input Set으로 다시 만들어 시간을 절약함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오리지날 Input Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_train = pd.read_json('input/train.json')\n",
    "original_test = pd.read_json('input/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data Set에서 Image 크기와 채도 명도가 추가되어 있는 Input Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_train = pd.read_json('input/train_with_image.json')\n",
    "original_test = pd.read_json('input/test_with_image.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image정보에 Building ID 0인 값을 채운 Input Data Set\n",
    "Building ID 0인 값은 위도 경도로 집계하였을 때 Building ID가 0을 제외하고 Unique하면 0값을 Unique한 Building ID로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_train = pd.read_json('input/filled_building_id_train.json')\n",
    "original_test = pd.read_json('input/filled_building_id_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위도 경도를 +-0.005로 잘라서 그 지역의 Price 시세를 구하고 High, Medium, Low의 수량으로 인기지역인지 판단한 정보가 추가되어 있는 Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_train2 = pd.read_json('input/train_add_price.json')\n",
    "original_test2 = pd.read_json('input/test_add_price.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge = merge_train_test(original_train,original_test)\n",
    "merge2 = merge_train_test(original_train2,original_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기본적인 Feature를 추가하는 함수**<br>\n",
    ">1. Kernel을 보면서 여러가지 Feature들을 추가\n",
    ">2. Data를 관찰하면서 새로운 Feature들을 추가\n",
    "\n",
    "<br>**Base Kernel**<br>\n",
    ">* https://www.kaggle.com/visnaga/two-sigma-connect-rental-listing-inquiries/xgboost-for-the-millionth-time-0-54724-lb\n",
    ">* https://www.kaggle.com/rakhlin/another-python-version-of-it-is-lit-by-branden\n",
    ">* https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-2-connect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature(df):\n",
    "    data = df.copy()\n",
    "    data['date'] = pd.to_datetime(data['created'])\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['wday'] = data['date'].dt.dayofweek\n",
    "    data['yday'] = data['date'].dt.dayofyear\n",
    "    data['hour'] = data['date'].dt.hour\n",
    "    data[\"total_days\"] =   (data[\"month\"] -4.0)*30 + data[\"day\"] +  data[\"hour\"] /25.0\n",
    "    data[\"diff_rank\"]= data[\"total_days\"]/data[\"listing_id\"]\n",
    "    \n",
    "    # 사진개수\n",
    "    data[\"photo_count\"] = data[\"photos\"].apply(len)\n",
    "\n",
    "    # 가격에 대한 추가 Feature\n",
    "    data[\"pricePerBed\"] = data['price'] / data['bedrooms']\n",
    "    data[\"pricePerBath\"] = data['price'] / data['bathrooms']\n",
    "    data[\"pricePerRoom\"] = data['price'] / (data['bedrooms'] + data['bathrooms'])\n",
    "    data[\"price_latitue\"] = (data[\"price\"])/ (data[\"latitude\"]+1.0)\n",
    "    data[\"price_longtitude\"] = (data[\"price\"])/ (data[\"longitude\"]-1.0)\n",
    "    #data[\"num_price_by_furniture\"] = data[\"price\"] / (data[\"bathrooms\"] + data[\"bedrooms\"])\n",
    "    \n",
    "    # 방에 대한 추가 Feature\n",
    "    data[\"bedPerBath\"] = data['bedrooms'] / data['bathrooms']\n",
    "    data[\"bedPerDiff\"] = data['bedrooms'] - data['bathrooms']\n",
    "    data[\"num_furniture\"] =  data[\"bathrooms\"] + data[\"bedrooms\"]\n",
    "    data[\"num_furniture\"] = data[\"num_furniture\"].apply(lambda x:  str(x) if float(x)<9.5 else '10')\n",
    "    data['num_furniture'] = data['num_furniture'].astype('float')\n",
    "    \n",
    "    # https://www.kaggle.com/ogrellier/median-rental-prices-matter/notebook/notebook\n",
    "    median_prices = data[['price','bedrooms']]\n",
    "    medians_by_key = median_prices.groupby(by='bedrooms')['price'].median().reset_index()\n",
    "    medians_by_key.rename(columns={'price': 'median_price_bed'}, inplace=True)\n",
    "    data = data.merge(medians_by_key, on='bedrooms', how='left')\n",
    "    data['price_ratio_bed_price_median'] = data['price'] / data['median_price_bed']\n",
    "    \n",
    "    data['zero_building_id'] = data['building_id'].apply(lambda x: 1 if x == '0' else 0)\n",
    "\n",
    "    #bc_price, tmp = boxcox(data.price)\n",
    "    #data['bc_price'] = bc_price\n",
    "    \n",
    "    data = data.fillna(-1).replace(np.inf, -1)\n",
    "    \n",
    "    data = convert_display_address(data)\n",
    "    data['empty_display_address'] = data['display_address'].apply(lambda x: 1 if x=='' else 0)\n",
    "    data['empty_street_address'] = data['street_address'].apply(lambda x: 1 if x=='' else 0)\n",
    "    \n",
    "    data = add_rank_feature(data,'building_id')\n",
    "    data = add_rank_feature(data,'manager_id')\n",
    "    \n",
    "    #data['longi_lati'] = data.apply(lambda row: str(row['longitude'])+ str(\"_\") + str(row['latitude']),axis=1)\n",
    "    \n",
    "    data = category_combine(data)\n",
    "    data = manager_id_diff_median_price(data)\n",
    "    data = image_timestamp(data)\n",
    "    data = add_city_distance(data)\n",
    "    data = add_floor_plan(data)\n",
    "    data = add_how_many_time_same_building(data)\n",
    "    data = add_how_many_time_same_display(data)\n",
    "    #data = calculate_manager_id_distance(data)\n",
    "    data = add_market_price(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.cluster import Birch\n",
    "def cluster_latlon(n_clusters, df):  \n",
    "    data = df.copy()\n",
    "    #split the data between \"around NYC\" and \"other locations\" basically our first two clusters \n",
    "    data_c=data[(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n",
    "    data_e=data[~((data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9))]\n",
    "    #put it in matrix form\n",
    "    coords=data_c.as_matrix(columns=['latitude', \"longitude\"])\n",
    "    \n",
    "    brc = Birch(branching_factor=100, n_clusters=n_clusters, threshold=0.01,compute_labels=True)\n",
    "\n",
    "    brc.fit(coords)\n",
    "    clusters=brc.predict(coords)\n",
    "    data_c[\"cluster_\"+str(n_clusters)]=clusters\n",
    "    data_e[\"cluster_\"+str(n_clusters)]=-1 #assign cluster label -1 for the non NYC listings \n",
    "    data=pd.concat([data_c,data_e])\n",
    "    plt.scatter(data_c[\"longitude\"], data_c[\"latitude\"], c=data_c[\"cluster_\"+str(n_clusters)], s=10, linewidth=0.1)\n",
    "    plt.title(str(n_clusters)+\" Neighbourhoods from clustering\")\n",
    "    plt.show()\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/adamsfei/only-brand-new-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def cart2rho(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    return rho\n",
    "\n",
    "\n",
    "def cart2phi(x, y):\n",
    "    phi = np.arctan2(y, x)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def rotation_x(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return x*math.cos(alpha) + y*math.sin(alpha)\n",
    "\n",
    "\n",
    "def rotation_y(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return y*math.cos(alpha) - x*math.sin(alpha)\n",
    "\n",
    "\n",
    "def add_rotation(degrees, df):\n",
    "    namex = \"rot\" + str(degrees) + \"_X\"\n",
    "    namey = \"rot\" + str(degrees) + \"_Y\"\n",
    "\n",
    "    df['num_' + namex] = df.apply(lambda row: rotation_x(row, math.pi/(180/degrees)), axis=1)\n",
    "    df['num_' + namey] = df.apply(lambda row: rotation_y(row, math.pi/(180/degrees)), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def operate_on_coordinates(tr_df, te_df):\n",
    "    for df in [tr_df, te_df]:\n",
    "        #polar coordinates system\n",
    "        df[\"num_rho\"] = df.apply(lambda x: cart2rho(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        df[\"num_phi\"] = df.apply(lambda x: cart2phi(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        #rotations\n",
    "        for angle in [15,30,45,60]:\n",
    "            df = add_rotation(angle, df)\n",
    "\n",
    "    return tr_df, te_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31962"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cap_share(x):\n",
    "    return sum(1 for c in x if c.isupper())/float(len(x)+1)\n",
    "\n",
    "def operate_description_featre(train_df,test_df):\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "    \n",
    "    for df in [train, test]:\n",
    "        # do you think that users might feel annoyed BY A DESCRIPTION THAT IS SHOUTING AT THEM?\n",
    "        df['num_cap_share'] = df['description'].apply(cap_share)\n",
    "\n",
    "        # how long in lines the desc is?\n",
    "        df['num_nr_of_lines'] = df['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "\n",
    "        # is the description redacted by the website?        \n",
    "        df['num_redacted'] = 0\n",
    "        df['num_redacted'].ix[df['description'].str.contains('website_redacted')] = 1\n",
    "\n",
    "        df['num_exclamation'] =  df['description'].apply(lambda x: x.count('!'))\n",
    "        # can we contact someone via e-mail to ask for the details?\n",
    "        df['num_email'] = 0\n",
    "        df['num_email'].ix[df['description'].str.contains('@')] = 1\n",
    "\n",
    "        #and... can we call them?\n",
    "\n",
    "        reg = re.compile(\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", re.S)\n",
    "        def try_and_find_nr(description):\n",
    "            if reg.match(description) is None:\n",
    "                return 0\n",
    "            return 1\n",
    "\n",
    "        df['num_phone_nr'] = df['description'].apply(try_and_find_nr)\n",
    "        \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_how_many_time_same_building(df):\n",
    "    data = df.copy()\n",
    "    temp = data.groupby(by=['manager_id','building_id','bedrooms','bathrooms'])['interest_level'].size().reset_index()\n",
    "    temp.rename(columns={0:'how_many_time_same_building'},inplace=True)\n",
    "    data = data.merge(temp,on=['manager_id','building_id','bedrooms','bathrooms'],how='left')\n",
    "    del temp\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_how_many_time_same_display(df):\n",
    "    data = df.copy()\n",
    "    temp = data.groupby(by=['manager_id','display_address'])['interest_level'].size().reset_index()\n",
    "    temp.rename(columns={0:'how_many_time_same_display_address'},inplace=True)\n",
    "    data = data.merge(temp,on=['manager_id','display_address'],how='left')\n",
    "    del temp\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "floor plan(평면도)을 detecting하여 csv로 파일로 만들어 놓음<br>\n",
    "listingid로 merge하여 사용함<br>\n",
    "**floor plan을 detecting하는 방법**\n",
    ">1. Image파일을 읽어서 사용한 색의 개수를 Count함\n",
    ">2. 사용한 색의 개수의 표준편차를 구하여 표준편차가 큰 것은 floor plan\n",
    ">3. 대각선의 색을 가져와 사용한 색의 개수가 적으면 floor plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_floor_plan(df):\n",
    "    data = df.copy()\n",
    "    floor_plan = pd.read_csv('input/floor_pan.csv')\n",
    "    \n",
    "    data = data.merge(floor_plan,on='listing_id',how='left')\n",
    "    del floor_plan\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newyork 시의 중심에서 부터의 거리 추가<br>\n",
    "https://www.kaggle.com/enrique1500/rental-listing-ny-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny_lat = 40.785091\n",
    "ny_lon = -73.968285\n",
    "\n",
    "def distance(row):\n",
    "    return np.sqrt((row['latitude'] - ny_lat)**2 + (row['longitude']-ny_lon)**2)\n",
    "\n",
    "def add_city_distance(df):\n",
    "    data = df.copy()\n",
    "    data['city_distance'] = data.apply(lambda row: distance(row),axis=1 )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manager_id로 집계하여 여러가지 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_manager_id_distance(df):\n",
    "    data = df.copy()\n",
    "    \n",
    "    manager_long_lati_median = data.groupby(by='manager_id')[['longitude','latitude']].median().reset_index()\n",
    "    manager_long_lati_median.rename(columns={'longitude': 'mid_long_median','latitude':'mid_lati_median'}, inplace=True)\n",
    "    data = data.merge(manager_long_lati_median,on='manager_id',how='left')\n",
    "    \n",
    "    data['x_diff'] = (data['mid_long_median'] - data['longitude'])**2\n",
    "    data['y_diff'] = (data['mid_lati_median'] - data['latitude'])**2\n",
    "    data['mid_distance'] = np.sqrt(data['x_diff'] + data['y_diff'])\n",
    "    \n",
    "    del manager_long_lati_median, data['mid_long_median'], data['mid_lati_median'],data['x_diff'],data['y_diff']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge magic timestamp feature**<br>\n",
    "이번 Kaggle에서는 Image 분석을 위하여 각 방의 Image를 제공하였는데 제공된 Image의 폴더 생성시간이 magic(leak) feature로 누출됨<br>\n",
    "0.001에서도 점수가 갈리는 대회에서 0.015점 정도가 향상되어 막판에 대회가 매우 치열해짐<br>\n",
    "https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_timestamp(df):\n",
    "    data = df.copy()\n",
    "    \n",
    "    leakage_image = pd.read_csv('listing_image_time.csv')\n",
    "    data = data.merge(leakage_image,on='listing_id',how='left')\n",
    "    \n",
    "    del leakage_image\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manager_id로 만든 파생변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def manager_id_diff_median_price(df):\n",
    "    data = df.copy()\n",
    "    manager_id_price_median = data.groupby(by='manager_id')['price'].median().reset_index()\n",
    "    manager_id_price_median.rename(columns={'price': 'mid_price_median'}, inplace=True)\n",
    "    data = data.merge(manager_id_price_median,on='manager_id',how='left')\n",
    "    \n",
    "    manager_set = pd.DataFrame(data['manager_id'].value_counts()).reset_index()\n",
    "    manager_set.rename(columns={'index':'manager_id','manager_id':'mid_count'},inplace=True)\n",
    "    data = data.merge(manager_set,on='manager_id',how='left')\n",
    "    \n",
    "    mid_disp_price_median = data.groupby(by=('manager_id','display_address'))['price'].median().reset_index()\n",
    "    mid_disp_price_median.rename(columns={'price': 'mid_disp_price_median'}, inplace=True)\n",
    "    data = data.merge(mid_disp_price_median,on=('manager_id','display_address'),how='left')\n",
    "\n",
    "    mid_building_price_median = data.groupby(by=('manager_id','building_id'))['price'].median().reset_index()\n",
    "    mid_building_price_median.rename(columns={'price': 'mid_building_price_median'}, inplace=True)\n",
    "    data = data.merge(mid_building_price_median,on=('manager_id','building_id'),how='left')\n",
    "\n",
    "    #data['price_ratio_mid_price_median'] = data['price'] / data['mid_price_median']\n",
    "    #data['price_ratio_mid_disp_price_median'] = data['price'] / data['mid_disp_price_median']\n",
    "    #data['price_ratio_mid_building_price_median'] = data['price'] / data['mid_building_price_median']\n",
    "    #data['diff_manager_price'] = data['manager_price_median'] - data['price']\n",
    "    del manager_set,manager_id_price_median,mid_building_price_median,mid_disp_price_median\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제를 제출한 renthop site에 방 타입별로 얼마나 비싼지가 나와있어서 feature로 추가함<br>\n",
    "https://www.renthop.com/nyc/apartments-for-rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_display_roomtype_price(df):\n",
    "    data = df.copy()\n",
    "    \n",
    "    data['bedroom_cat'] = data['bedrooms'].apply(lambda x: 4 if x>=4 else x)\n",
    "    data['bathroom_cat'] = data['bathrooms'].apply(lambda x: 5 if x>=5 else x)\n",
    "    \n",
    "    disp_roomtype_price_median = data.groupby(by=('display_address','fea_studio','fea_loft','bedroom_cat','bathroom_cat'))['price'].median().reset_index()\n",
    "    disp_roomtype_price_median.rename(columns={'price': 'disp_roomtype_price_median'}, inplace=True)\n",
    "    \n",
    "    data = data.merge(disp_roomtype_price_median,on=('display_address','fea_studio','fea_loft','bedroom_cat','bathroom_cat'),how='left')\n",
    "    data['how_many_expansive'] = data['price'] - data['disp_roomtype_price_median']\n",
    "    \n",
    "    del data['bedroom_cat'],data['bathroom_cat']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위도 경도 +-0.005 지역을 정하여 각 지역마다 가격의 시세를 정하였고 그 지역이 인기 있는 지역인지 feature를 추가함<br>\n",
    "CV Score는 좋았지만 LB Score가 좋지 않아 결국 사용하지 않음(overfitting 됨)<br>\n",
    "**LB Score는 Kaggle에서 Submission을 하였을 때 나오는 Score를 말함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_market_price(df):\n",
    "    data = df.copy()\n",
    "    #unique_longi_lat_df = data.copy()\n",
    "    data['longi_lati'] = data.apply(lambda row: str(row['longitude'])+ str(\"_\") + str(row['latitude']),axis=1)\n",
    "    unique_longi_lat_df = data.copy()\n",
    "    unique_longi_lat_df = unique_longi_lat_df.drop_duplicates(subset='longi_lati')\n",
    "\n",
    "    for idx,row in unique_longi_lat_df.iterrows():\n",
    "        lati = row['latitude']\n",
    "        longi = row['longitude']\n",
    "        near_place_df = data.ix[(data['latitude']>lati-0.005)&(data['latitude']<lati+0.005)&(data['longitude']<longi+0.005)&(data['longitude']>longi-0.005)].copy()\n",
    "        \n",
    "        low_df = near_place_df.ix[near_place_df['interest_level']=='low']\n",
    "        unique_longi_lat_df.set_value(idx,'place_low_median',low_df['price'].median())\n",
    "        low_cnt = low_df.shape[0]\n",
    "        \n",
    "        medium_df = near_place_df.ix[near_place_df['interest_level']=='medium']\n",
    "        unique_longi_lat_df.set_value(idx,'place_medium_median',medium_df['price'].median())\n",
    "        medi_cnt = medium_df.shape[0]\n",
    "        \n",
    "        high_df = near_place_df.ix[near_place_df['interest_level']=='high']\n",
    "        unique_longi_lat_df.set_value(idx,'place_high_median',high_df['price'].median())\n",
    "        high_cnt = high_df.shape[0]\n",
    "        \n",
    "        all_cnt = low_cnt+medi_cnt+high_cnt\n",
    "        \n",
    "        unique_longi_lat_df.set_value(idx,'market_price',near_place_df['price'].median())\n",
    "        popular = 0\n",
    "        if all_cnt != 0:\n",
    "            popular = (medi_cnt+2*high_cnt)/all_cnt\n",
    "            \n",
    "        unique_longi_lat_df.set_value(idx,'popularity_place',popular)\n",
    "        \n",
    "        del row,near_place_df,low_df,medium_df,high_df\n",
    "    \n",
    "    temp = unique_longi_lat_df[['longi_lati','market_price','popularity_place','place_low_median','place_medium_median','place_high_median']]\n",
    "    data = data.merge(temp, on='longi_lati', how='left')\n",
    "    del unique_longi_lat_df,temp\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 Kernel을 Manager의 Level을 추가함<br>\n",
    "https://www.kaggle.com/guoday/cv-statistics-better-parameters-and-explaination/notebook/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def manager_level(train,test):\n",
    "    train_df = train.copy()\n",
    "    test_df = test.copy()\n",
    "    \n",
    "    index=list(range(train_df.shape[0]))\n",
    "    random.shuffle(index)\n",
    "    a=[np.nan]*len(train_df)\n",
    "    b=[np.nan]*len(train_df)\n",
    "    c=[np.nan]*len(train_df)\n",
    "\n",
    "    for i in range(5):\n",
    "        building_level={}\n",
    "        for j in train_df['manager_id'].values:\n",
    "            building_level[j]=[0,0,0]\n",
    "        test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "        train_index=list(set(index).difference(test_index))\n",
    "        for j in train_index:\n",
    "            temp=train_df.iloc[j]\n",
    "            if temp['interest_level']=='low':\n",
    "                building_level[temp['manager_id']][0]+=1\n",
    "            if temp['interest_level']=='medium':\n",
    "                building_level[temp['manager_id']][1]+=1\n",
    "            if temp['interest_level']=='high':\n",
    "                building_level[temp['manager_id']][2]+=1\n",
    "        for j in test_index:\n",
    "            temp=train_df.iloc[j]\n",
    "            if sum(building_level[temp['manager_id']])!=0:\n",
    "                a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "                b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "                c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "    train_df['manager_level_low']=a\n",
    "    train_df['manager_level_medium']=b\n",
    "    train_df['manager_level_high']=c\n",
    "\n",
    "    a=[]\n",
    "    b=[]\n",
    "    c=[]\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    for j in range(train_df.shape[0]):\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "    for i in test_df['manager_id'].values:\n",
    "        if i not in building_level.keys():\n",
    "            a.append(np.nan)\n",
    "            b.append(np.nan)\n",
    "            c.append(np.nan)\n",
    "        else:\n",
    "            a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "            b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "            c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "    test_df['manager_level_low']=a\n",
    "    test_df['manager_level_medium']=b\n",
    "    test_df['manager_level_high']=c\n",
    "\n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def category_combine(data):\n",
    "    lencat=len(categorical)\n",
    "    for f in range (0,lencat):\n",
    "        for s in range (f+1,lencat):\n",
    "            new_category = categorical[f] + \"_\" +categorical[s]\n",
    "            if new_category == 'display_address_street_address':\n",
    "                continue\n",
    "                \n",
    "            data[new_category] =data[categorical[f]]+\"_\" + data[categorical[s]]\n",
    "            categorical.append(new_category)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_category_combine_list(ct):\n",
    "    category = ct.copy()\n",
    "    lencat=len(category)\n",
    "    for f in range (0,lencat):\n",
    "        for s in range (f+1,lencat):\n",
    "            new_category = category[f] + \"_\" +category[s]\n",
    "            if new_category == 'display_address_street_address':\n",
    "                continue\n",
    "                \n",
    "            category.append(new_category)\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "address_map = {\n",
    "    'w': 'west',\n",
    "    'st.': 'street',\n",
    "    'ave': 'avenue',\n",
    "    'st': 'street',\n",
    "    'e': 'east',\n",
    "    'n': 'north',\n",
    "    's': 'south'\n",
    "}\n",
    "\n",
    "new_address_cols = ['street', 'avenue', 'east', 'west', 'north', 'south']\n",
    "\n",
    "def address_map_func(s):\n",
    "    s = s.split(' ')\n",
    "    out = []\n",
    "    for x in s:\n",
    "        if x in address_map:\n",
    "            out.append(address_map[x])\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return ' '.join(out)\n",
    "\n",
    "def convert_display_address(data):\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    data[\"display_address\"] = data[\"display_address\"].apply(fmt)\n",
    "    data['display_address'] = data['display_address'].apply(lambda x: x.translate(remove_punct_map))\n",
    "    data['display_address'] = data['display_address'].apply(lambda x: address_map_func(x))\n",
    "    \n",
    "    data[\"street_address\"] = data[\"street_address\"].apply(fmt)\n",
    "    data['street_address'] = data['street_address'].apply(lambda x: x.translate(remove_punct_map))\n",
    "    data['street_address'] = data['street_address'].apply(lambda x: address_map_func(x))\n",
    "    \n",
    "    for col in new_address_cols:\n",
    "        data[col] = data['display_address'].apply(lambda x: 1 if col in x else 0)\n",
    "        \n",
    "    data['other_address'] = data[new_address_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_rank_feature(data,col):\n",
    "    val = [99,98,95,90,85,80,75,70,50]\n",
    "    column = ['top1','top2','top5','top10','top15','top20','top25','top30','top50']\n",
    "    attributes = zip(val,column)\n",
    "    \n",
    "    value_count = data[col].value_counts()\n",
    "    \n",
    "    for val,column_name in attributes:\n",
    "        top_column_name = column_name + \"_\"  + col\n",
    "        upper_value = np.percentile(value_count.values, 90)\n",
    "        upper_index = value_count.index.values[value_count.values >= upper_value ]\n",
    "        data[top_column_name] = data[col].apply(lambda x: 1 if x in upper_index else 0)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def designate_single_observations(df1, df2, column):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    grouped = ps.groupby(ps).size().to_frame().rename(columns={0: \"size\"})\n",
    "    new_column = column +\"_exist_one\"\n",
    "    df1[new_column] = 0\n",
    "    df1.loc[df1.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, new_column] = 1\n",
    "    df2[new_column] = 0\n",
    "    df2.loc[df2.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, new_column] = 1\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hcc_encode(train_df, test_df, variable, target, prior_prob, k, f=1, g=1, r_k=None, update_df=None):\n",
    "    \"\"\"\n",
    "    See \"A Preprocessing Scheme for High-Cardinality Categorical Attributes in\n",
    "    Classification and Prediction Problems\" by Daniele Micci-Barreca\n",
    "    \"\"\"\n",
    "    hcc_name = \"_\".join([\"hcc\", variable, target])\n",
    "\n",
    "    grouped = train_df.groupby(variable)[target].agg({\"size\": \"size\", \"mean\": \"mean\"})\n",
    "    grouped[\"lambda\"] = 1 / (g + np.exp((k - grouped[\"size\"]) / f))\n",
    "    grouped[hcc_name] = grouped[\"lambda\"] * grouped[\"mean\"] + (1 - grouped[\"lambda\"]) * prior_prob\n",
    "\n",
    "    df = test_df[[variable]].join(grouped, on=variable, how=\"left\")[hcc_name].fillna(prior_prob)\n",
    "    if r_k: df *= np.random.uniform(1 - r_k, 1 + r_k, len(test_df))     # Add uniform noise. Not mentioned in original paper\n",
    "\n",
    "    if update_df is None: update_df = test_df\n",
    "    if hcc_name not in update_df.columns: update_df[hcc_name] = np.nan\n",
    "    update_df.update(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string.punctuation.__add__('!!')\n",
    "string.punctuation.__add__('(')\n",
    "string.punctuation.__add__(')')\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def cleaning_text(sentence):\n",
    "    sentence=re.sub('\\d+',' ', sentence) #removes digits\n",
    "    cleaned=' '.join([w for w in sentence.split() if not w in stop]) # removes english stopwords\n",
    "    #cleaned=' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    #selecting only nouns and adjectives\n",
    "    cleaned=' '.join([w for w in cleaned.split() if not len(w)<=2 ]) #removes single lettered words and digits\n",
    "    cleaned=cleaned.strip()\n",
    "    return cleaned\n",
    "\n",
    "def convert_description(data):\n",
    "    data['desc'] = data['description']\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    #fmt = lambda s: s.strip().lower()\n",
    "    data['desc'] = data['desc'].apply(fmt)\n",
    "    data['desc'] = data['desc'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n",
    "    data['desc'] = data['desc'].apply(lambda x: x.replace('!<br /><br />', ''))\n",
    "    data['desc'] = data['desc'].apply(lambda x: x.translate(remove_punct_map))\n",
    "    data['desc'] = data['desc'].apply(lambda x: cleaning_text(x))\n",
    "    stem_word = nltk.stem.SnowballStemmer('english')\n",
    "    data['desc'] = data['desc'].apply(lambda x: stem_word.stem(x))\n",
    "    \n",
    "    data[\"desc_wordcount\"] = data[\"desc\"].apply(str.split).apply(len)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string.punctuation.__add__('!!')\n",
    "string.punctuation.__add__('(')\n",
    "string.punctuation.__add__(')')\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "def replace_similar_word(s):\n",
    "    x = s.replace(\"no fee\", \"nofee\")\n",
    "    x = x.replace(\"no-fee\", \"nofee\")\n",
    "    x = x.replace(\"no  fee\", \"nofee\")\n",
    "    x = x.replace(\"no_fee\", \"nofee\")\n",
    "    \n",
    "    x = x.replace(\"reduced_fee\", \"lowfee\")\n",
    "    x = x.replace(\"low_fee\", \"lowfee\")\n",
    "    x = x.replace(\"reduced_fee\", \"lowfee\")\n",
    "    x = x.replace(\"low fee\", \"lowfee\")\n",
    "    \n",
    "    x = x.replace(\"hardwood\", \"parquet\")\n",
    "    \n",
    "    x = x.replace(\"concierge\", \"doorman\")\n",
    "    x = x.replace(\"housekeep\", \"doorman\")\n",
    "    x = x.replace(\"in_super\", \"doorman\")\n",
    "    \n",
    "    x = x.replace(\"pre_war\", \"prewar\")\n",
    "    x = x.replace(\"pre war\", \"prewar\")\n",
    "    x = x.replace(\"pre-war\", \"prewar\")\n",
    "    \n",
    "    x = x.replace(\"laundry\", \"lndry\")\n",
    "    \n",
    "    x = x.replace(\"gym\", \"health\")\n",
    "    x = x.replace(\"fitness\", \"health\")\n",
    "    x = x.replace(\"training\", \"health\")\n",
    "    \n",
    "    x = x.replace(\"train\", \"transport\")\n",
    "    x = x.replace(\"subway\", \"transport\")\n",
    "    \n",
    "    x = x.replace(\"subway\", \"transport\")\n",
    "    \n",
    "    x = x.replace(\"twenty four hour\", \"24\")\n",
    "    x = x.replace(\"24/7\", \"24\")\n",
    "    x = x.replace(\"24hr\", \"24\")\n",
    "    x = x.replace(\"24-hour\", \"24\")\n",
    "    x = x.replace(\"24hour\", \"24\")\n",
    "    x = x.replace(\"24 hour\", \"24\")\n",
    "    x = x.replace(\"common\", \"cm\")\n",
    "    \n",
    "    x = x.replace(\"bicycle\", \"bike\")\n",
    "    \n",
    "    x = x.replace(\"private\", \"pv\")\n",
    "    x = x.replace(\"decorative\", \"deco\")\n",
    "    x = x.replace(\"onsite\", \"os\")\n",
    "    x = x.replace(\"outdoor\", \"od\")\n",
    "    x = x.replace(\"ss appliances\", \"stainless\")\n",
    "    return x\n",
    "    \n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def cleaning_text(sentence):\n",
    "    sentence=re.sub('\\d+',' ', sentence) #removes digits\n",
    "    cleaned=' '.join([w for w in sentence.split() if not w in stop]) # removes english stopwords\n",
    "    #cleaned=' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    #selecting only nouns and adjectives\n",
    "    cleaned=' '.join([w for w in cleaned.split() if not len(w)<=2 ]) #removes single lettered words and digits\n",
    "    cleaned=cleaned.strip()\n",
    "    return cleaned\n",
    "\n",
    "def convert_feature(data):\n",
    "    data['feature'] = data['features']\n",
    "    data[\"num_features\"] = data[\"feature\"].apply(len)\n",
    "    data['feature']=data['feature'].apply(lambda x: ', '.join(x))\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    data['feature'] = data['feature'].apply(fmt)\n",
    "    data['feature'] = data['feature'].apply(lambda x: x.translate(remove_punct_map))\n",
    "    data['feature'] = data['feature'].apply(lambda x: cleaning_text(x))\n",
    "    data['feature'] = data['feature'].apply(lambda x: replace_similar_word(x))\n",
    "    \n",
    "    \n",
    "    stem_word = nltk.stem.SnowballStemmer('english')\n",
    "    data['feature'] = data['feature'].apply(lambda x: stem_word.stem(x))\n",
    "    \n",
    "    data['feature'] = data['feature'].apply(lambda x: x.strip())\n",
    "    #data[\"feature_wordcount\"] = data[\"feature\"].apply(str.split).apply(len)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_train_test(data):\n",
    "    train_df = data.ix[:train_row-1,:]\n",
    "    test_df = data.ix[train_row:,:]\n",
    "    \n",
    "    del_column = ['interest_level','pred_0', 'pred_1', 'pred_2']\n",
    "    for col in del_column:\n",
    "        if col in test_df.columns:\n",
    "            del test_df[col]\n",
    "\n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_train_test(train_df,test_df):\n",
    "    data = pd.concat((train_df, test_df), axis=0).reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_column(data,col_list):\n",
    "    data = data.drop(col_list, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = original_train.copy()\n",
    "test_df = original_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49352\n"
     ]
    }
   ],
   "source": [
    "train_row = train_df.shape[0]\n",
    "print(train_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listing_id = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df = merge_train_test(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\",\"street_address\"]\n",
    "merged_df = add_feature(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Data Set에 place정보가 추가되어 있을 경우만 사용함 fillna place price feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del merged_df['longi_lati']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "place_high_median_median = merged_df.ix[~merged_df['place_high_median'].isnull()]['place_high_median'].median()\n",
    "merged_df.loc[merged_df['place_high_median'].isnull(),'place_high_median'] = place_high_median_median\n",
    "\n",
    "place_medium_median_median = merged_df.ix[~merged_df['place_medium_median'].isnull()]['place_medium_median'].median()\n",
    "merged_df.loc[merged_df['place_medium_median'].isnull(),'place_medium_median'] = place_medium_median_median\n",
    "\n",
    "place_low_median_median = merged_df.ix[~merged_df['place_low_median'].isnull()]['place_low_median'].median()\n",
    "merged_df.loc[merged_df['place_low_median'].isnull(),'place_low_median'] = place_low_median_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 빌딩 ID Merge - 성능이 좋아지지 않아 마지막에는 사용하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20664, 19)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.ix[merged_df['building_id']=='0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "building_id_train = pd.read_json('input/filled_building_id_train.json')\n",
    "building_id_test = pd.read_json('input/filled_building_id_test.json')\n",
    "building_merge = merge_train_test(building_id_train,building_id_test)\n",
    "merged_df.loc[merged_df['building_id']=='0','building_id']=np.nan\n",
    "\n",
    "temp = building_merge[['listing_id','building_id']]\n",
    "\n",
    "merged_df = merged_df.merge(temp,on='listing_id',how='left')\n",
    "del merged_df['building_id_x']\n",
    "merged_df.rename(columns={'building_id_y':'building_id'},inplace=True)\n",
    "\n",
    "del temp,building_id_train,building_id_test,building_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4507, 19)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.ix[merged_df['building_id']=='0'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### no - use find single category feature and make new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = separate_train_test(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Special designation for building_ids, manager_ids, display_address with only 1 observation\n",
    "for col in ('building_id', 'manager_id', 'display_address','street_address'):\n",
    "    train_df, test_df = designate_single_observations(train_df, test_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### called cv statistics almost same feature hcc encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = separate_train_test(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df,test_df = manager_level(train_df,test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coordinate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = operate_on_coordinates(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### description feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = operate_description_featre(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### newyork cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df = merge_train_test(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_only_long_lati = merged_df[['longitude','latitude','listing_id']]\n",
    "merged_only_long_lati = cluster_latlon(14,merged_only_long_lati)\n",
    "merged_df = merged_df.merge(merged_only_long_lati[['listing_id','cluster_14']],on='listing_id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df = merge_train_test(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df = convert_description(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### description 변환은 현재 사용하지 않음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfdesc=TfidfVectorizer(min_df=20, max_features=50, lowercase =True,\n",
    "                        analyzer='word', ngram_range=(1, 2), use_idf=False,smooth_idf=False, \n",
    "    sublinear_tf=True, stop_words = 'english')\n",
    "\n",
    "tfidfdesc.fit(merged_df['desc'])\n",
    "\n",
    "desc_sparse = tfidfdesc.transform(merged_df['desc'])\n",
    "\n",
    "desc_sparse_cols = tfidfdesc.get_feature_names()\n",
    "desc_col_list = []\n",
    "for desc_col in desc_sparse_cols:\n",
    "    desc_col_list.append('desc'+'_'+desc_col)\n",
    "    \n",
    "desc_sparse_df = pd.DataFrame(desc_sparse.toarray(),columns=desc_col_list)\n",
    "merged_df = pd.concat([merged_df,desc_sparse_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del_col = []\n",
    "for col in merged_df.columns:\n",
    "    if (col.find('desc_')!=-1) & (col != 'desc_wordcount'):\n",
    "        del_col.append(col)\n",
    "        \n",
    "for col in del_col:\n",
    "    del merged_df[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature 특성 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df = convert_feature(merged_df)\n",
    "\n",
    "c_vect = CountVectorizer(stop_words='english', max_features=200, ngram_range=(1, 1))\n",
    "c_vect.fit(merged_df['feature'])\n",
    "\n",
    "c_vect_sparse_1 = c_vect.transform(merged_df['feature'])\n",
    "\n",
    "c_vect_sparse1_cols = c_vect.get_feature_names()\n",
    "feature_col_list = []\n",
    "for desc_col in c_vect_sparse1_cols:\n",
    "    feature_col_list.append('fea'+'_'+desc_col)\n",
    "    \n",
    "feature_sparse_df = pd.DataFrame(c_vect_sparse_1.toarray(),columns=feature_col_list)\n",
    "\n",
    "merged_df = pd.concat([merged_df,feature_sparse_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add display roomtype price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df = add_display_roomtype_price(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del merged_df['desc'],merged_df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del merged_df['feature'],merged_df['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueTooSmallError(Exception):\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "try:\n",
    "    if len(categorical) <5:\n",
    "        raise ValueTooSmallError\n",
    "except NameError:\n",
    "    categorical = [\"display_address\", \"manager_id\", \"building_id\",\"street_address\"]\n",
    "    categorical = get_category_combine_list(categorical)\n",
    "except ValueTooSmallError:\n",
    "    categorical = get_category_combine_list(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feat in categorical:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(merged_df[feat].values))\n",
    "    merged_df[feat] = lbl.transform(list(merged_df[feat].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change interest_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49352\n"
     ]
    }
   ],
   "source": [
    "train_row = train_df.shape[0]\n",
    "print(train_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df, test_df = separate_train_test(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.replace({\"interest_level\": {\"low\": 0, \"medium\": 1, \"high\": 2}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_drop_col_list = ['date', 'created','year','photos']\n",
    "test_drop_col_list = ['date', 'created','year','photos']\n",
    "train_df = drop_column(train_df,train_drop_col_list)\n",
    "test_df = drop_column(test_df,test_drop_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in train_df.columns:\n",
    "    if (col.find('longi_')!=-1):\n",
    "        print(col)\n",
    "        del train_df[col]\n",
    "        del test_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = train_df.interest_level.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del train_df['interest_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_df.copy()\n",
    "x_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 279)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 279)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.to_csv('x_train_14th.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test.to_csv('x_test_14th.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train,columns=['interest_level']).to_csv('y_train_14th.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listing_id = x_test.listing_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(listing_id,columns = ['listing_id']).to_csv('test_listing_14th.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
